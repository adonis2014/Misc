BACKGROUND: 
    Downside of existing system:
        <1> Distributed File System: Diffulty in handling large number of objects
        <2> Database and K-V Store : Diffulty in handling large sized objects

An in-house solution

GOALs:
<1> Highly Available and horizontally scalable
<2> Low operational overhead
<3> Lower MTTR (Mean time to repair)
<4> Active-Active Setup across multiple data centers (DCs)
<5> Efficient for large and small media objects
<6> Cheap
<7> Non-blocking and stream pipeline: better throughput and latency

[remark]: Active-Active across DC is the ability to enable updates to the same object from different data centers
          The router library is currently blocking (synchronous) but we are actively working to provide a non-blocking (async) version

Storage:
    Quick writes and fewer metadata operation
    Asynchronous batched log flush
    Leave more room for page cache - great for read of recent data
    Bloom filters for lookups
    Zero copy reads
    
High Availability:
    No master or slaves -  active-active setup
    Geo-distributed
    Replication is asynchronous
    Replicas are not binary identical
    Reconstruction a replica does not stress a single server
    Dedicated system resouces for communication b/w (between or within) a pair of DCs
    Eventually consistency (but GET after PUT consistency because of cross datacenter request)
    
Horizontal Scalability
    Generation of IDs at the router ensures good distribution of data + requests
    Router is stateless - adding one is easy
    Adding partition and replicas is fairly easy
    Cluster expansion has no effects on replication at an individual server
    Restricted failure domains
    Adding partitions and replicas has no side effects.
        no rebalancing required - no data movement
        no range/cluster metadata changes required at the Router
    Server interactions scale perfectly
        There is an absolute bound on the number of peer servers
        Number of messages in the cluster don't grow exponentially 

Support for small and large objects:
    Low metadata lookup overhead at Storage tier
    Structure of storage, help data structure and page cache minimize disk operations
    Chunking at Router tier to support large object
    Streaming asynchronous components keep memory usage in check
    Easy to apply back pressure to keep memory usage in check
    Large object support is storage agnostic (storage这层是不知道大文件还是小文件，因为chunk是在router那层做的)
    Chunking is good for throughput, fairness, distribution, being resilient to occasional quorum failure

Use Helix for cluster management
Small objects is supported by composing it as a single simple blob ??

The router will consist of a set of scaling units. A call that comes to the router will be internally assigned to one of the scaling units, 
which will handle the request and the response for that call. The scaling units work independently of each other. 

use Netty as the NIO framework.

/******************************** Ambry Multiproducts and Services ***************************************/
Ambry:               Library wrapper for open source code
AmbryLI:             Deployable that includes the library and some closed source code
ambry-migrationtool: Contains a sanity test suite
ambry-client:        Used by clients to access Ambry

/****************************************** Concepts **********************************************/
Clustermap: controls the topology, maintains resource states and helps coordinate cluster operations.
            clustermap = hardware layout + partition layout
Frontends:  receive and route request
Datanodes (storage node):  store the actual data. Each datanode manages a number of disks
                           hosts replicas for different partitions. Typically, 
                           each storage node has N disks across which the replicas are distributed.
                           
Cluster Managers: maintain the states of cluster including partition state, partition location, disk placement etc.
                  Cluster Managers are sync up by Helix

Datacenter = Frontends + Datanodes + Cluster Managers (Helix to coordinate)

blob: large immutable data / large media objects

proxy request: forward requests to other data centers when data is not replicated in current datacenter.

partition: a logical group of blobs, current size is 100GB (at first ready-write then it becomes read-only)
           also, partition is a logical slice of the cluster
           a list of replicas that can span across data centers 
           any data rebalancing across the cluster happens at the partition level.
           Partition 是逻辑概念，物理上的placement是另一个流程。partition是以append-only log的形式实现的，存于 pre-allocted large file中

Indexing:  each Datanode maintains a light-weight in-memory indexing per replica;
           indexing has several sorted segments (from new to old)
           indexing is in memory, if datanode fails, it can be recovered from on-disk partition
            
CDN = Content Delivery Network

Router Library:  choose a partition, communicate with Datanode in charge and serve the request.

change capture system: capture events changes out of Ambry for offline analysis

/****************************************** Components **********************************************/
(1) Cluster Manager:
                    Hardware Layer:
                                   <1> arrangement of datacenter, datanode, disks
                                   <2> record raw capacity of each disk
                                   <3> monitor healthy or failed of each disk
                                  
                    Logical Layer:
                                   <1> maintain location of partition replica
                                   <2> periodically check state of each partition: read-only / read-write
                                   <3> partition replica can placed on multiple datacenter / multiple datanode
                                   <4> onde disk can contain replicas of different partitions

(2) Frontends (decentralized, stateless):
                  Responsibility:
                                  <1> Handle request (route via Router Library)
                                  <2> Check security (authentication)
                                  <3> Capturing Operations (change capture system)
                                  <4> Failure detection
                  Router Library:
                                  <1> policy based routing: random for PUT;  retrieve blob id for GET/DELETE
                                  <2> chunking blobs: split large blob to equal-size chunks (4 ~ 8 MB)
                                  <3> zero-cost Failure detection leveraging request message (no more ACK/PING)
                                  <4> proxy request handle read after write
                                  
    Interfaces in frontend
    (2.1) Interaction enablers:  enable the different components to interact with each other in a way that is 
                                 agnostic to the underlying implementations of each of the components. 
    (2.2) ReadableStreamChannel: Through this interface, data can be streamed (in the form of bytes) between the 
                                 interacting pieces as if reading it through a channel (asynchronous reads)
    (2.3) RestRequest: extends the ReadableStreamChannel and is implemented by the NIO layer.
    (2.4) RestResponseChannel: implemented by the NIO layer, provides a way for the remote service and 
                               scaling layers to return processed responses to the client.
    
(3) Datanode Layer:
                    <1> Blob Indexing: made up of segments; per partition replica; blob id map to start offset
                    <2> OS Caching: latest segment stays in memory. 
                                    Flush to disk when exceed maximum size. 
                                    Each segment has a bloom filter.
                                    Reverse lookup index segment when reading blob
                    <3> Batch Writes: single disk seek
                    <4> Zero Copy: kernel directly copies data from disk to network buffer without going through the application.
                    
/****************************************** Data Structure **********************************************/
(1)PUT/DELETE entry: header
                     blob id = parition id(8 bytes) + UUID(32 bytes)
                     blob size
                     TTL = time to live
                     creation time
                     content type
                     delete flag

(2)Datanode:     indexing of blobs
                 journals
                 bloom filters (reduce lookup latency for on-disk index segments)


(3)Indexing of blobs:  Index segment1, Index segment2, Index segment3 ... 
                       each segment:  <blob id, start offset> pairs + in-memory bloom filter
                      
(4)Chunking blob: (Ambry split large blob into smaller equal-size chunks with size 4~8MB. The chunks are placed on different paritions)
                  blob metadata = # of chunks + (ChunkId1 + paritionId1 + UUID1) + (ChunkId2 + paritionId2 + UUID2) + ...
                  (blob metadata is viewed as a normal blob and is put into Ambry)
                  use sliding buffer of size s to retrieve the blob

(5)Journal:  each partition replica has a journal. A in-memory cache of recent blobs ordered by offset
             partition p replica r1 journal = offset + blob id (track the latest Offset)
 
/****************************************** Mechanism **********************************************/
(1) Load Balance: (two metrics for load balance: disk usage + request rate)
                  (read-write partition receive all write requests and major read requests)
                  Ideal State:  
                                idealRW = ideal # of read-write partitions per disk = (# of R-W partitions) / (# of disks)
                                idealRO = ideal # of read-only partition per disk = (# of RO partition) / (# of disks)
                                idealUsed = ideal disk usage each disk should have
                  Two-phase Rebalance:
                                // Phase1: move extra partitions into a partition pool. 
                                partitionPool = {}
                                for each disk d do
                                    // Move extra read-write partitions. 
                                    while d.NumRW > idealRW do
                                        partitionPool += chooseMinimumUsedRW(d) //选择这个disk中容量最小的partition，这样会最小化data movement          
                                    // Move extra read-only partitions.
                                    while d.NumRO > idealRO & d.used > idealUsed do //同时考虑RO partition 和 disk usage
                                        partitionPool += chooseRandomRO(d)      //随机选一个partition
                   
                                 // Phase2: Move partitions to disks needing partitions.
                                 placePartitions(read-write) 
                                 placePartitions(read-only)
                                 
                                 function placePartitions(Type t)
                                    while partitionPool contains partitions type t do
                                        D=shuffleDisksBelowIdeal()       // shuffle 一下，使用随机round-robin方法
                                        for disk d in D and partition p in pool do
                                              d.addPartition(p) 
                                              partitionPool.remove(p)
                   Place Replica:
                                  <1> create a new replica in the destination
                                  <2> sync up new replica with old replica
                                  <3> serve new write on both new and old replica
                                  <4> delete old replica after sync


(2) Replica Consistency:
                        Features: <1> periodically synchronize replicas
                                  <2> decentralize all-to-all fashion: each replica acts as a master and sync up with other replicas
                                  <3> asynchronous 2-phase replication protocol
                                  <4> dedicated threads for lagging replicas
                                  <5> batching requests and batching blobs a
                                  datacenters
                                  <6> use different thread pool for inter- and intra- datacenter replication respectively
                                  
                        Two-phase Replication Protocol:
                                  <1> find missing blobs since last sync point (request all blob ids from others from last sync point and filter out missing ones locally)
                                  <2> send request for missing blobs only, receive them and append to replica

(3) Failure Detection:
              Goal:  Find unavailable datanodes/disks avoid forwarding requests to them
              <1> Record # of consecutive failed requests for specific datanode/disk. (During last CHECK_PERIOD time)
              <2> If # of failed requests >= MAX_FAIL(= 2), mark datanode as temp_down for WAIT_PERIOD time
              <3> After WAIT_PERIOD time, another request is sent to this node: mark temp_down if fails; mark available if succeed.

(4) Service API, K = 2 policy:
            PUT: request is forwarded to all replicas, policies define the # of acknowledgements needed for a success.
                 put is performed synchronously only in the local datacenter. Later, blob is replicated to other datacenters.
            GET: policies define how many randomly selected replicas to contact.
