/****************************************** Concepts **********************************************/
Frontends:  receive and route request
Datanodes:  store the actual data. Each datanode manages a number of disks
Cluster Managers: maintain the states of cluster including partition state, partition location, disk placement etc.

Datacenter = Frontends + Datanodes + Cluster Managers (Zookeeper to coordinate)

blob: large immutable data / large media objects

proxy request: forward requests to other data centers when data is not replicated in current datacenter.

partition: a logical group of blobs, current size is 100GB (at first ready-write then it becomes read-only)

CDN = Content Delivery Network

Router Library:  choose a partition, communicate with Datanode in charge and serve the request.


/****************************************** Data Structure **********************************************/
(1)PUT/DELETE entry: header
                     blob id = parition id(8 bytes) + UUID(32 bytes)
                     blob size
                     TTL = time to live
                     creation time
                     content type
                     delete flag

(2)Datanode:     indexing of blobs
                 journals
                 bloom filters (reduce lookup latency for on-disk index segments)


(3)Indexing of blobs:  Index segment1, Index segment2, Index segment3 ... 
                       each segment:  <blob id, start offset> pairs + in-memory bloom filter
                      
(4)Chunking blob: (Ambry split large blob into smaller equal-size chunks with size 4~8MB. The chunks are placed on different paritions)
                  blob metadata = # of chunks + (ChunkId1 + paritionId1 + UUID1) + (ChunkId2 + paritionId2 + UUID2) + ...
                  (blob metadata is viewed as a normal blob and is put into Ambry)
                  use sliding buffer of size s to retrieve the blob

(5)Journal:  each partition replica has a journal. A in-memory cache of recent blobs ordered by offset
             partition p replica r1 journal = offset + blob id (track the latest Offset)
 
/****************************************** Mechanism **********************************************/
(1) Load Balance: (two metrics for load balance: disk usage + request rate)
                  (read-write partition receive all write requests and major read requests)
                  Ideal State:  
                                idealRW = ideal # of read-write partitions per disk = (# of R-W partitions) / (# of disks)
                                idealRO = ideal # of read-only partition per disk = (# of RO partition) / (# of disks)
                                idealUsed = ideal disk usage each disk should have
                  Two-phase Rebalance:
                                // Phase1: move extra partitions into a partition pool. 
                                partitionPool = {}
                                for each disk d do
                                    // Move extra read-write partitions. 
                                    while d.NumRW > idealRW do
                                        partitionPool += chooseMinimumUsedRW(d) //选择这个disk中容量最小的partition，这样会最小化data movement          
                                    // Move extra read-only partitions.
                                    while d.NumRO > idealRO & d.used > idealUsed do //同时考虑RO partition 和 disk usage
                                        partitionPool += chooseRandomRO(d)      //随机选一个partition
                   
                                 // Phase2: Move partitions to disks needing partitions.
                                 placePartitions(read-write) 
                                 placePartitions(read-only)
                                 
                                 function placePartitions(Type t)
                                    while partitionPool contains partitions type t do
                                        D=shuffleDisksBelowIdeal()       // shuffle 一下，使用随机round-robin方法
                                        for disk d in D and partition p in pool do
                                              d.addPartition(p) 
                                              partitionPool.remove(p)
                   Place Replica:
                                  <1> create a new replica in the destination
                                  <2> sync up new replica with old replica
                                  <3> serve new write on both new and old replica
                                  <4> delete old replica after sync


(2) Replica Consistency:
                        Features: <1> periodically synchronize replicas
                                  <2> decentralize all-to-all fashion: each replica acts as a master and sync up with other replicas
                                  <3> asynchronous 2-phase replication protocol
                                  <4> dedicated threads for lagging replicas
                                  <5> batching requests and batching blobs across datacenters
                                  <6> use different thread pool for inter- and intra- datacenter replication respectively
                                  
                        Two-phase Replication Protocol:
                                  <1> find missing blobs since last sync point (request all blob ids from others from last sync point and filter out missing ones locally)
                                  <2> send request for missing blobs only, receive them and append to replica

(3) Failure Detection:
              Goal:  Find unavailable datanodes/disks avoid forwarding requests to them
              <1> Record # of consecutive failed requests for specific datanode/disk. (During last CHECK_PERIOD time)
              <2> # of failed requests >= MAX_FAIL(= 2) 
